diff --git a/kernel/locking/qspinlock.c b/kernel/locking/qspinlock.c
index 6f5d518..577588e 100644
--- a/kernel/locking/qspinlock.c
+++ b/kernel/locking/qspinlock.c
@@ -198,7 +198,11 @@ static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)
  */
 static __always_inline void clear_pending(struct qspinlock *lock)
 {
+#ifdef FIX5
+    atomic_fetch_andnot_release(-_Q_PENDING_VAL + _Q_LOCKED_VAL, &lock->val);
+#else
 	atomic_andnot(_Q_PENDING_VAL, &lock->val);
+#endif
 }
 
 /**
@@ -209,7 +213,11 @@ static __always_inline void clear_pending(struct qspinlock *lock)
  */
 static __always_inline void clear_pending_set_locked(struct qspinlock *lock)
 {
-	atomic_add(-_Q_PENDING_VAL + _Q_LOCKED_VAL, &lock->val);
+#ifdef FIX4
+    atomic_fetch_add_release(-_Q_PENDING_VAL + _Q_LOCKED_VAL, &lock->val);
+#else
+    atomic_add(-_Q_PENDING_VAL + _Q_LOCKED_VAL, &lock->val);
+#endif
 }
 
 /**
@@ -233,7 +241,11 @@ static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)
 		 * the MCS node is properly initialized before updating the
 		 * tail.
 		 */
-		old = atomic_cmpxchg_relaxed(&lock->val, val, new);
+#ifdef FIX2
+		old = atomic_cmpxchg_release(&lock->val, val, new);
+#else
+        old = atomic_cmpxchg_relaxed(&lock->val, val, new);
+#endif
 		if (old == val)
 			break;
 
@@ -388,7 +400,12 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 	 *
 	 * 0,0,* -> 0,1,* -> 0,0,1 pending, trylock
 	 */
-	val = queued_fetch_set_pending_acquire(lock);
+#ifdef FIX3
+    val = atomic_fetch_or_release(_Q_PENDING_VAL, &lock->val);
+    smp_mb();
+#else
+    val = queued_fetch_set_pending_acquire(lock);
+#endif
 
 	/*
 	 * If we observe contention, there is a concurrent locker.
@@ -470,8 +487,8 @@ pv_queue:
 	 */
 	barrier();
 
-	node->locked = 0;
-	node->next = NULL;
+    WRITE_ONCE(node->locked, 0);
+    WRITE_ONCE(node->next, NULL);
 	pv_init_node(node);
 
 	/*
@@ -489,7 +506,11 @@ pv_queue:
 	 * publish the updated tail via xchg_tail() and potentially link
 	 * @node into the waitqueue via WRITE_ONCE(prev->next, node) below.
 	 */
-	smp_wmb();
+#ifdef FIX1
+	smp_mb();
+#else
+    smp_wmb();
+#endif
 
 	/*
 	 * Publish the updated tail.
