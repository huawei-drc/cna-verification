diff --git a/include/asm-generic/qspinlock.h b/include/asm-generic/qspinlock.h
index d74b138..09a365e 100644
--- a/include/asm-generic/qspinlock.h
+++ b/include/asm-generic/qspinlock.h
@@ -68,7 +68,11 @@ static __always_inline int queued_spin_trylock(struct qspinlock *lock)
 	return likely(atomic_try_cmpxchg_acquire(&lock->val, &val, _Q_LOCKED_VAL));
 }
 
+#if defined(CONFIG_NUMA_AWARE_SPINLOCKS)
+extern void __cna_queued_spin_lock_slowpath(struct qspinlock *lock, u32 val);
+#else
 extern void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val);
+#endif /* defined(CONFIG_NUMA_AWARE_SPINLOCKS) */
 
 #ifndef queued_spin_lock
 /**
@@ -82,7 +86,12 @@ static __always_inline void queued_spin_lock(struct qspinlock *lock)
 	if (likely(atomic_try_cmpxchg_acquire(&lock->val, &val, _Q_LOCKED_VAL)))
 		return;
 
-	queued_spin_lock_slowpath(lock, val);
+#if defined(CONFIG_NUMA_AWARE_SPINLOCKS)
+	/* We bypass the call (ignoring the boot argument). */
+	__cna_queued_spin_lock_slowpath(lock, val);
+#else
+	queued_spin_lock_slowpath(lock, val);
+#endif /* defined(CONFIG_NUMA_AWARE_SPINLOCKS) */
 }
 #endif
 
@@ -96,7 +105,11 @@ static __always_inline void queued_spin_unlock(struct qspinlock *lock)
 	/*
 	 * unlock() needs release semantics:
 	 */
+#ifdef VERIFICATION
+	atomic_fetch_sub_release(_Q_LOCKED_VAL, &lock->val);
+#else
 	smp_store_release(&lock->locked, 0);
+#endif
 }
 #endif
 
diff --git a/kernel/locking/mcs_spinlock.h b/kernel/locking/mcs_spinlock.h
index 3926aad..82afef1 100644
--- a/kernel/locking/mcs_spinlock.h
+++ b/kernel/locking/mcs_spinlock.h
@@ -110,7 +110,7 @@ void mcs_spin_unlock(struct mcs_spinlock **lock, struct mcs_spinlock *node)
 		if (likely(cmpxchg_release(lock, node, NULL) == node))
 			return;
 		/* Wait until the next pointer is set */
-		while (!(next = READ_ONCE(node->next)))
+		await_while(!(next = READ_ONCE(node->next)))
 			cpu_relax();
 	}
 
diff --git a/kernel/locking/qspinlock.c b/kernel/locking/qspinlock.c
index 8c1a21b..6f5d518 100644
--- a/kernel/locking/qspinlock.c
+++ b/kernel/locking/qspinlock.c
@@ -66,7 +66,9 @@
  */
 
 #include "mcs_spinlock.h"
-#define MAX_NODES	4
+/* NOTE: In the verification, we do not consider interrupts, so we can reduce
+ * the number of nodes per core to 1. */
+#define MAX_NODES	1
 
 /*
  * On 64-bit architectures, the mcs_spinlock structure will be 16 bytes in
@@ -141,7 +143,7 @@ struct mcs_spinlock *grab_mcs_node(struct mcs_spinlock *base, int idx)
 
 #define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)
 
-#if _Q_PENDING_BITS == 8
+#if _Q_PENDING_BITS == 8 && !defined(VERIFICATION)
 /**
  * clear_pending - clear the pending bit.
  * @lock: Pointer to queued spinlock structure
@@ -263,7 +265,11 @@ static __always_inline u32 queued_fetch_set_pending_acquire(struct qspinlock *lo
  */
 static __always_inline void set_locked(struct qspinlock *lock)
 {
+#ifdef VERIFICATION
+	atomic_or(_Q_LOCKED_VAL, &lock->val);
+#else
 	WRITE_ONCE(lock->locked, _Q_LOCKED_VAL);
+#endif
 }
 
 
@@ -348,7 +354,9 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 	struct mcs_spinlock *prev, *next, *node;
 	u32 old, tail;
 	int idx;
-
+#ifdef SKIP_PENDING
+	goto queue;
+#endif
 	BUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));
 
 	if (pv_enabled())
@@ -471,8 +479,10 @@ pv_queue:
 	 * attempt the trylock once more in the hope someone let go while we
 	 * weren't watching.
 	 */
+#if !defined(SKIP_PENDING)
 	if (queued_spin_trylock(lock))
 		goto release;
+#endif
 
 	/*
 	 * Ensure that the initialisation of @node is complete before we
diff --git a/kernel/locking/qspinlock_cna.h b/kernel/locking/qspinlock_cna.h
index 17d56c7..c828781 100644
--- a/kernel/locking/qspinlock_cna.h
+++ b/kernel/locking/qspinlock_cna.h
@@ -74,7 +74,9 @@ static inline bool intra_node_threshold_reached(struct cna_node *cn)
 	u64 current_time = local_clock();
 	u64 threshold = cn->start_time + numa_spinlock_threshold_ns;
 
-	return current_time > threshold;
+	/* NOTE: we non-deterministically decide whether the intra-node threshold
+	 * was reached in the client code. */
+	return READ_ONCE(cna_threshold_reached); //current_time > threshold;
 }
 
 /*
@@ -96,6 +98,9 @@ static DEFINE_PER_CPU(u32, seed);
  */
 static bool probably(unsigned int num_bits)
 {
+#ifdef VERIFICATION
+	return 0;
+#else
 	u32 s;
 
 	s = this_cpu_read(seed);
@@ -103,6 +108,7 @@ static bool probably(unsigned int num_bits)
 	this_cpu_write(seed, s);
 
 	return s & ((1 << num_bits) - 1);
+#endif
 }
 
 static void __init cna_init_nodes_per_cpu(unsigned int cpu)
@@ -111,6 +117,7 @@ static void __init cna_init_nodes_per_cpu(unsigned int cpu)
 	int numa_node = cpu_to_node(cpu);
 	int i;
 
+	__VERIFIER_loop_bound(MAX_NODES+1);
 	for (i = 0; i < MAX_NODES; i++) {
 		struct cna_node *cn = (struct cna_node *)grab_mcs_node(base, i);
 
@@ -136,6 +143,7 @@ static int __init cna_init_nodes(void)
 	/* we store an ecoded tail word in the node's @locked field */
 	BUILD_BUG_ON(sizeof(u32) > sizeof(unsigned int));
 
+	__VERIFIER_loop_bound(NTHREADS+1);
 	for_each_possible_cpu(cpu)
 		cna_init_nodes_per_cpu(cpu);
 
@@ -327,8 +335,11 @@ static __always_inline u32 cna_wait_head_or_lock(struct qspinlock *lock,
 		 * Try and put the time otherwise spent spin waiting on
 		 * _Q_LOCKED_PENDING_MASK to use by sorting our lists.
 		 */
+		/* NOTE: to speed up verification, we reorder the queue only twice.
 		while (LOCK_IS_BUSY(lock) && !cna_order_queue(node))
-			cpu_relax();
+			cpu_relax(); */
+		cna_order_queue(node);
+		cna_order_queue(node);
 	} else {
 		cn->start_time = FLUSH_SECONDARY_QUEUE;
 	}
@@ -376,6 +387,8 @@ static inline void cna_lock_handoff(struct mcs_spinlock *node,
 	arch_mcs_lock_handoff(&next->locked, val);
 }
 
+/* NOTE: The remainder of the files isn't necessary for verification */
+#if 0
 /*
  * Constant (boot-param configurable) flag selecting the NUMA-aware variant
  * of spinlock.  Possible values: -1 (off) / 0 (auto, default) / 1 (on).
@@ -423,3 +436,4 @@ void __init cna_configure_spin_lock_slowpath(void)
 
 	pr_info("Enabling CNA spinlock\n");
 }
+#endif
